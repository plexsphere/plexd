{
  "feature_id": "PXD-0001",
  "title": "A001: Control Plane Client (HTTPS + SSE)",
  "slug": "a001-control-plane-client-https-sse",
  "status": "proposed",
  "phase": null,
  "summary": "",
  "description": "\n\n**Size:** ðŸ—ï¸ large\n**Category:** backend\n**Priority:** critical\n\n## Summary\n\nS001 implements the foundational outbound-only communication layer between plexd and the Plexsphere control plane. It provides two transport mechanisms: an HTTPS client for request/response API calls (registration, heartbeat, state fetch, metrics delivery, etc.) and a persistent SSE client for receiving real-time events (peer changes, policy updates, action requests, key rotation). This is the lowest-level building block â€” every other plexd feature (S002â€“S022) depends on it.\n\n## Scope\n\n**Included:**\n- `ControlPlane` client struct using `net/http` stdlib â€” shared HTTP client, base URL, TLS config, injectable auth token (`Authorization: Bearer`), gzip, JSON ser/de\n- All 17 control plane API endpoint methods as typed Go functions (per README API reference: `POST /v1/register` through `GET /v1/artifacts/...`)\n- SSE client connecting to `GET /v1/nodes/{node_id}/events` with `text/event-stream` line-by-line parsing\n- SSE reconnection via `ReconnectEngine` â€” exponential backoff (1s â†’ 60s cap) with Â±25% jitter\n- `Last-Event-ID` tracking and replay header on reconnect\n- Event dispatching to registered handlers by `event_type` string (all 11 event types from README)\n- Signed event envelope parsing â€” JSON struct: `event_type`, `event_id`, `issued_at`, `nonce`, `payload`, `signature`\n- `EventVerifier` interface for signature + staleness + nonce checks (pluggable; concrete Ed25519 impl deferred to S010)\n- Polling fallback after 5 minutes continuous SSE failure (`GET /v1/nodes/{id}/state`)\n- Structured error types per HTTP status (`ErrUnauthorized`, `ErrConflict`, `ErrNotFound`, etc.)\n- Context-based cancellation and graceful shutdown of SSE stream\n- Configuration loading for `api` block (base URL, TLS, timeouts) from config file / env / flags\n- Unit tests with `httptest.Server` for all HTTPS endpoints and SSE stream scenarios (reconnect, backoff, dispatch, envelope parsing)\n\n**Excluded (YAGNI / belongs to other features):**\n- Bootstrap token reading from file/env/metadata â€” S002\n- Registration orchestration logic (retry state machine) â€” S002\n- Reconciliation loop scheduling and diff logic â€” S003\n- Ed25519 signature verification implementation â€” S010\n- Nonce tracking bounded set implementation â€” S010 (interface defined here)\n- Heartbeat scheduling â€” S003\n- WireGuard or mesh-layer logic â€” S005\n- Metrics/log/audit batch delivery scheduling â€” S016/S017/S018\n- Node API server â€” S004\n\n## Visualization\n\n```mermaid\nflowchart TD\n    subgraph plexd[\"plexd process\"]\n        CFG[\"Config Loader\"]\n        CP[\"ControlPlane Client\"]\n        HTTP[\"HTTPS Transport\"]\n        SSE[\"SSE Stream Manager\"]\n        DISP[\"Event Dispatcher\"]\n        VERIF[\"EventVerifier interface\"]\n        RECON[\"Reconnect Engine\"]\n\n        CFG -->|\"base URL, TLS\"| CP\n        CP --> HTTP\n        CP --> SSE\n        SSE --> RECON\n        SSE -->|\"raw events\"| VERIF\n        VERIF -->|\"verified events\"| DISP\n        DISP -->|\"by event_type\"| HANDLERS[\"Handler Registry\"]\n    end\n\n    subgraph controlplane[\"Plexsphere Control Plane\"]\n        REG_API[\"Registration API\"]\n        HB_API[\"Heartbeat API\"]\n        STATE_API[\"State API\"]\n        EVBUS[\"Event Bus / SSE\"]\n        OTHER[\"Other Endpoints\"]\n    end\n\n    HTTP -->|\"POST, GET, PUT\"| REG_API\n    HTTP -->|\"POST\"| HB_API\n    HTTP -->|\"GET, POST\"| STATE_API\n    HTTP -->|\"various\"| OTHER\n    EVBUS -->|\"text/event-stream\"| SSE\n```\n\n```mermaid\nsequenceDiagram\n    participant P as plexd\n    participant CP as Control Plane\n\n    P->>CP: POST /v1/register\n    CP-->>P: 201 node_id, signing_key, peers\n\n    P->>CP: GET /v1/nodes/{id}/events\n    CP-->>P: 200 text/event-stream\n\n    loop SSE Stream\n        CP--)P: event: peer_added (signed envelope)\n        P->>P: Parse envelope, verify signature\n        P->>P: Dispatch to handler\n    end\n\n    Note over P,CP: Connection drops\n    P->>P: Backoff 1s, 2s, 4s... +/- 25% jitter\n    P->>CP: GET /v1/nodes/{id}/events (Last-Event-ID: evt_xyz)\n    CP-->>P: Replay missed events + resume stream\n\n    Note over P,CP: SSE down > 5 min\n    loop Polling Fallback\n        P->>CP: GET /v1/nodes/{id}/state\n        CP-->>P: Full state snapshot\n    end\n```\n\n```mermaid\nstateDiagram-v2\n    [*] --> Disconnected\n    Disconnected --> Connecting: Start\n    Connecting --> Connected: SSE 200 OK\n    Connecting --> Backoff: Connection failed\n    Backoff --> Connecting: Timer expired\n    Connected --> Processing: Event received\n    Processing --> Connected: Event dispatched\n    Connected --> Disconnected: Connection lost\n    Backoff --> Polling: 5 min timeout\n    Polling --> Connecting: SSE retry\n    Polling --> Polling: Poll interval\n    Connected --> [*]: Shutdown\n    Polling --> [*]: Shutdown\n```\n\n## Key Components\n\n- **`internal/api/client.go`** â€” `ControlPlane` struct: shared `*http.Client`, base URL, TLS config, auth token. Exposes typed methods for all 17 API endpoints (`Register()`, `Heartbeat()`, `FetchState()`, `ReportMetrics()`, etc.). Handles JSON marshalling, gzip, error mapping.\n- **`internal/api/sse.go`** â€” `SSEStream` struct: manages persistent `GET /v1/nodes/{id}/events` connection, parses `text/event-stream` line protocol (`event:`, `data:`, `id:`, `retry:`), tracks `Last-Event-ID`.\n- **`internal/api/reconnect.go`** â€” `ReconnectEngine`: exponential backoff (1sâ€“60s) with Â±25% jitter, 5-minute cumulative SSE failure threshold triggering polling fallback, automatic SSE retry from polling state.\n- **`internal/api/dispatcher.go`** â€” `EventDispatcher`: type-safe handler registry, routes parsed events to registered `EventHandler` functions keyed by `event_type` string.\n- **`internal/api/envelope.go`** â€” `SignedEnvelope` struct: JSON parsing for envelope schema (`event_type`, `event_id`, `issued_at`, `nonce`, `payload`, `signature`). Defines `EventVerifier` interface with `Verify(envelope) error`.\n- **`internal/api/errors.go`** â€” Structured error types mapping HTTP status codes: `ErrUnauthorized` (401), `ErrForbidden` (403), `ErrNotFound` (404), `ErrConflict` (409), `ErrRateLimit` (429), `ErrServer` (5xx).\n- **`internal/api/config.go`** â€” Client configuration struct: base URL, TLS skip-verify flag, timeouts (connect, request, SSE idle), loaded from `/etc/plexd/config.yaml` `api` block.\n- **`internal/api/*_test.go`** â€” Unit tests using `httptest.Server` for all HTTPS endpoints and SSE stream simulation (reconnect, backoff, event dispatch, envelope parsing, error handling).",
  "stories": [],
  "requirements": [],
  "tasks": [],
  "test_specifications": [],
  "affected_files": [],
  "similar_patterns": [],
  "review_criteria": [],
  "implementation_notes": "",
  "status_history": {
    "draft": {
      "github_account": "berendt",
      "timestamp": "2026-02-11T20:39:25.103985"
    },
    "elaborated": {
      "github_account": "berendt",
      "timestamp": "2026-02-11T21:32:08.213849"
    }
  },
  "origin": "plan",
  "devils_advocate_report": {
    "verdict": "RECONSIDER",
    "summary": "The proposal is directionally sound â€” a control plane client is genuinely the foundation for everything else. However, it suffers from premature over-specification, scope creep disguised as \"foundational\" work, an untested API contract assumption, and several design decisions that lock in complexity before the first line of code is written. The core HTTPS client + SSE stream should proceed, but with roughly 40% of the scope deferred or simplified.",
    "challenges": [
      {
        "claim": "All 17 control plane API endpoint methods should be implemented as typed Go functions upfront",
        "counter_argument": "This is classic speculative generality. S001 is consumed by S002 (registration), S003 (heartbeat/reconciliation), and the SSE stream. Those features need maybe 4-5 endpoints: Register, Heartbeat, FetchState, the SSE events endpoint, and possibly ReportStatus. The remaining 12+ endpoints (ReportMetrics, FetchArtifact, PostAuditLog, etc.) are consumed by S016-S018 and others that are far down the roadmap. Writing typed methods for endpoints you won't call for months means: (a) you're coding against an API contract that will drift before consumers exist, (b) you're writing tests for code with no callers, (c) you're making the PR massive and hard to review. The generic `do(ctx, method, path, body, response)` helper that all 17 methods would use IS the real deliverable â€” the typed wrappers are trivial to add when each consuming feature lands.",
        "risk_level": "high",
        "alternative": "Implement the core HTTP transport layer (do/doJSON helpers, auth injection, error mapping, gzip) plus only the endpoints needed by S002 and S003 (Register, Heartbeat, FetchState, ReportStatus). Add remaining endpoint methods in the features that consume them. This cuts ~60% of the HTTPS client code and tests while delivering identical value to the next 3 features."
      },
      {
        "claim": "The API contract for all 17 endpoints is stable and known",
        "counter_argument": "The README is the only source of truth for the API reference, and READMEs are aspirational documents, not API specifications. There is no OpenAPI spec, no running control plane to test against, and no integration test environment mentioned. Coding typed request/response structs for 17 endpoints against a prose description guarantees rework. Every field name, every response shape, every error code is an assumption. The SSE envelope schema (event_type, event_id, issued_at, nonce, payload, signature) is similarly unvalidated â€” if the control plane team changes \"issued_at\" to \"timestamp\" or nests the signature differently, every test breaks.",
        "risk_level": "high",
        "alternative": "Define the transport layer and error handling generically. For the few endpoints needed immediately, accept that the types are provisional and mark them clearly. Consider generating client code from an OpenAPI spec once one exists rather than hand-rolling 17 endpoint methods that may all need revision."
      },
      {
        "claim": "SSE with exponential backoff + polling fallback is the right real-time communication pattern",
        "counter_argument": "SSE is a reasonable choice for server-to-client push over HTTP, but the proposal doesn't justify it over alternatives. Key concerns: (1) SSE is unidirectional and uses a long-lived HTTP/1.1 connection â€” if plexd sits behind a corporate proxy or load balancer with aggressive idle timeouts (common in edge/IoT deployments), the connection will be killed repeatedly, making the reconnect engine the hot path rather than an exception path. (2) The 5-minute polling fallback threshold is arbitrary â€” why not 2 minutes? Why not 30 seconds? No evidence or reasoning is provided. (3) The `Last-Event-ID` replay mechanism assumes the server implements event buffering with the same semantics â€” this is a server-side contract that isn't validated. (4) WebSocket or gRPC streaming would give bidirectional communication, eliminating the need for separate HTTPS endpoints for some operations, and gRPC in particular gives you typed contracts and code generation for free.",
        "risk_level": "medium",
        "alternative": "If SSE is mandated by the control plane's existing implementation, proceed but make the 5-minute threshold configurable rather than hardcoded. If the control plane doesn't exist yet, seriously evaluate gRPC bidirectional streaming â€” it eliminates the SSE+HTTPS duality, gives you typed contracts via protobuf, and handles reconnection via standard gRPC keepalive. Trade-off: gRPC adds a protobuf dependency but removes the entire SSE parser, reconnect engine, and polling fallback."
      },
      {
        "claim": "EventVerifier interface should be defined here with signature + staleness + nonce checks, even though the Ed25519 implementation is deferred to S010",
        "counter_argument": "Designing an interface before you have a single concrete implementation is premature abstraction. The interface definition encodes assumptions about what verification needs â€” you've decided it's signature + staleness + nonce as a single `Verify(envelope) error` call. But what if S010 discovers that verification needs to be async (fetching a rotated key from the control plane)? What if nonce checking requires shared state that doesn't fit a stateless interface? What if staleness needs configurable thresholds per event type? You'll either contort the implementation to fit the premature interface or change the interface and break S001's tests. The YAGNI principle the proposal itself invokes for other features applies equally here.",
        "risk_level": "medium",
        "alternative": "Parse the envelope and dispatch events without verification in S001. When S010 lands, it introduces the verifier interface shaped by actual implementation needs and wires it into the SSE pipeline. S001 can accept an optional verification hook (a simple function type, not an interface) that defaults to no-op. This is less code, fewer assumptions, and the interface emerges from real requirements."
      },
      {
        "claim": "The ReconnectEngine with exponential backoff (1s â†’ 60s cap) with Â±25% jitter is well-specified",
        "counter_argument": "The reconnection strategy conflates two failure modes that require different responses: (1) transient network blips (where aggressive reconnection is correct) and (2) server-side rejection like 401/403 (where reconnecting is pointless without re-authentication). The proposal doesn't describe how HTTP status codes from the SSE endpoint influence backoff behavior. If the SSE endpoint returns 401 because the token expired, exponential backoff to 60 seconds is wrong â€” you need to trigger token refresh and reconnect immediately, or stop entirely. Similarly, a 429 response should respect the Retry-After header, not use custom backoff. The state machine diagram shows no transition for \"authentication failure\" or \"permanent rejection\" â€” it just loops through Backoff forever.",
        "risk_level": "high",
        "alternative": "Classify SSE connection failures into: (a) retryable-transient (network error, 5xx â†’ exponential backoff), (b) retryable-with-action (401 â†’ trigger re-auth callback, then reconnect), (c) respect-server (429 â†’ use Retry-After header), (d) permanent (403, 404 â†’ stop and surface error). The reconnect engine should accept a classifier function and different strategies per category."
      },
      {
        "claim": "Configuration loading for the `api` block from config file / env / flags belongs in S001",
        "counter_argument": "Configuration loading is a cross-cutting concern. If S001 implements its own config parser for `/etc/plexd/config.yaml`, every subsequent feature (S002-S022) that needs configuration will either duplicate the parsing logic or depend on S001's config infrastructure, which violates the single-responsibility principle. Configuration loading should be a separate, minimal foundation (or part of the main binary's bootstrap) that all features consume, not owned by the API client package. Putting config loading inside `internal/api/` couples the API client to file I/O, YAML parsing, and environment variable reading â€” concerns that don't belong in an HTTP client library.",
        "risk_level": "medium",
        "alternative": "S001's `ControlPlane` client accepts a `Config` struct as a constructor argument. A separate `internal/config` package (or the main package) handles file/env/flag parsing and passes the populated config in. This keeps the API client pure and testable without filesystem dependencies, and the config infrastructure serves all features."
      },
      {
        "claim": "This is a single coherent feature that should be implemented as one unit",
        "counter_argument": "The proposal bundles three distinct capabilities: (1) HTTPS request/response client, (2) SSE streaming client with reconnection, (3) event envelope parsing and dispatch. These have different complexity profiles, different testing strategies, and different consumers. The HTTPS client is straightforward and well-understood. The SSE client with reconnection is a stateful, concurrent system that's notoriously tricky to test correctly (race conditions, timer mocking, partial reads). The event dispatcher is a simple registry pattern. Bundling them forces a massive PR that's hard to review, hard to bisect if something breaks, and delays delivering the HTTPS client (which S002 needs immediately) until the SSE reconnection engine is battle-tested.",
        "risk_level": "medium",
        "alternative": "Split into two increments: (A) HTTPS client with error types, core endpoint methods for S002/S003, and config struct â€” small, reviewable, unblocks S002 immediately. (B) SSE stream, reconnect engine, event dispatch, polling fallback â€” can be developed in parallel and merged when ready. Both live in `internal/api/` and share the HTTP client."
      },
      {
        "claim": "Custom SSE line-by-line parser is the right approach",
        "counter_argument": "The SSE spec (W3C EventSource) has subtle edge cases: multi-line data fields, BOM handling, comment lines (lines starting with `:` used as keepalives), CR vs LF vs CRLF line endings, empty event names defaulting to \"message\", and the retry field influencing reconnection timing. Hand-rolling a parser means hand-testing all these edge cases. There are well-tested Go SSE client libraries (e.g., `r3labs/sse`, `tmaxmax/go-sse`) that handle these correctly. The \"not invented here\" cost is real: you're writing and maintaining a parser for a specification that already has robust implementations.",
        "risk_level": "medium",
        "alternative": "Evaluate `tmaxmax/go-sse` or similar library for the SSE parsing layer. Wrap it with the custom reconnection logic and event verification. If the dependency is unacceptable (minimal dependency policy), at minimum write the parser against the W3C spec with comprehensive edge-case tests, not just the happy path."
      }
    ],
    "risk_flags": [
      "CRITICAL: No running control plane exists to validate the API contract â€” all 17 endpoint types are assumptions that will drift. Building typed wrappers now guarantees rework proportional to the number of endpoints implemented.",
      "CRITICAL: The reconnect engine has no failure classification â€” a 401 from the SSE endpoint triggers the same backoff as a network timeout. In production, an expired token will cause 5 minutes of useless reconnection attempts before falling back to polling (which will also fail with 401), creating a silent outage.",
      "HIGH: The proposal's scope (\"large\") underestimates the testing complexity of the SSE reconnect engine. Testing concurrent state machines with timers, partial reads, and race conditions in Go requires careful use of test clocks and synchronization. Budget for this or cut scope.",
      "MEDIUM: TLS skip-verify flag in configuration is a security footgun. If this is for development only, it should require an explicit \"I know what I'm doing\" flag (e.g., `PLEXD_INSECURE_SKIP_TLS=1`) and log a warning, not be a normal config option."
    ],
    "alternatives": [
      "**Phased delivery (recommended):** Phase 1 delivers HTTPS transport core (do/doJSON, error types, auth injection) + 4-5 endpoint methods needed by S002/S003 + config struct (not loader). Phase 2 delivers SSE stream + reconnect with failure classification + event dispatch. Phase 3 adds remaining endpoint methods as consuming features land. Trade-off: slightly more PRs, but each is reviewable, testable, and immediately useful. Unblocks S002 weeks earlier.",
      "**gRPC instead of HTTPS+SSE:** If the control plane API is not yet finalized, a single gRPC service with bidirectional streaming replaces both the HTTPS client and SSE client. Protobuf gives you typed contracts with code generation, streaming gives you real-time events, and gRPC's built-in keepalive/reconnection eliminates the custom reconnect engine entirely. Trade-off: requires protobuf toolchain, less familiar to some teams, harder to debug with curl. Only viable if the control plane hasn't committed to REST+SSE.",
      "**Use an existing SSE library:** Replace the custom SSE parser with a battle-tested library like `tmaxmax/go-sse` and focus implementation effort on the business-specific parts (envelope verification, event dispatch, reconnect policy). Trade-off: adds a dependency but eliminates ~200 lines of parser code and associated edge-case tests."
    ]
  }
}